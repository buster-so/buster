import { Agent, createStep } from '@mastra/core';
import type { CoreMessage } from 'ai';
import { wrapTraced } from 'braintrust';
import { z } from 'zod';
import {
  type assumptionItemSchema,
  listAssumptionsResponse,
} from '../../tools/post-processing/list-assumptions-response';
import { noAssumptionsIdentified } from '../../tools/post-processing/no-assumptions-identified';
import { MessageHistorySchema } from '../../utils/memory/types';
import { anthropicCachedModel } from '../../utils/models/anthropic-cached';
import { standardizeMessages } from '../../utils/standardizeMessages';

const inputSchema = z.object({
  conversationHistory: MessageHistorySchema.optional(),
  user_name: z.string().describe('Name for the post-processing operation'),
  messageId: z.string().describe('Message ID for the current operation'),
  userId: z.string().describe('User ID for the current operation'),
  chatId: z.string().describe('Chat ID for the current operation'),
  isFollowUp: z.boolean().describe('Whether this is a follow-up message'),
  previousMessages: z.array(z.string()).describe('Array of previous messages for context'),
  datasets: z.string().describe('Assembled YAML content of all available datasets for context'),
});

export const identifyAssumptionsOutputSchema = z.object({
  // Pass through all input fields
  conversationHistory: MessageHistorySchema.optional(),
  user_name: z.string().describe('Name for the post-processing operation'),
  messageId: z.string().describe('Message ID for the current operation'),
  userId: z.string().describe('User ID for the current operation'),
  chatId: z.string().describe('Chat ID for the current operation'),
  isFollowUp: z.boolean().describe('Whether this is a follow-up message'),
  previousMessages: z.array(z.string()).describe('Array of previous messages for context'),
  datasets: z.string().describe('Assembled YAML content of all available datasets for context'),

  // New fields from this step
  toolCalled: z.string().describe('Name of the tool that was called by the agent'),
  assumptions: z
    .array(
      z.object({
        descriptiveTitle: z.string().describe('A clear, descriptive title for the assumption'),
        classification: z
          .enum([
            'fieldMapping',
            'tableRelationship',
            'dataQuality',
            'dataFormat',
            'dataAvailability',
            'timePeriodInterpretation',
            'timePeriodGranularity',
            'metricInterpretation',
            'metricDefinition',
            'businessLogic',
            'segmentInterpretation',
            'segmentDefinition',
            'requestScope',
            'quantityInterpretation',
            'aggregation',
            'filtering',
            'sorting',
            'grouping',
            'calculationMethod',
            'dataRelevance',
            'dataInterpretation',
          ])
          .describe('The type/category of assumption made'),
        explanation: z
          .string()
          .describe('Detailed explanation of the assumption and its potential impact'),
        label: z
          .enum(['timeRelated', 'vagueRequest', 'major', 'minor'])
          .describe('Label indicating the nature and severity of the assumption'),
      })
    )
    .optional()
    .describe('List of assumptions identified'),
});

// Template function that accepts datasets parameter
const createIdentifyAssumptionsInstructions = (datasets: string): string => {
  return `
### Overview
You are a specialized AI agent within an AI-powered data analyst system. Your role is to analyze SQL queries generated by the data analyst (Buster) and identify any assumptions made during the query construction process.

### Your Task
Review the SQL queries and related context to identify assumptions that were made. These assumptions could significantly impact the accuracy or reliability of the results if they are incorrect.

### Types of Assumptions to Identify
Look for these categories of assumptions:

1. **Field Mapping Assumptions** (\`fieldMapping\`)
   - Assumptions about which database fields correspond to business concepts
   - Choosing between multiple similar fields without clear documentation

2. **Table Relationship Assumptions** (\`tableRelationship\`)
   - Assumptions about how tables should be joined
   - Inferred relationships not explicitly documented

3. **Data Quality Assumptions** (\`dataQuality\`)
   - Assumptions about data completeness, accuracy, or consistency
   - Filtering decisions based on data quality concerns

4. **Data Format Assumptions** (\`dataFormat\`)
   - Assumptions about data types, formats, or encoding
   - Date format interpretations or currency assumptions

5. **Data Availability Assumptions** (\`dataAvailability\`)
   - Assumptions about what data exists or doesn't exist
   - Proxy measurements when direct data isn't available

6. **Time Period Assumptions** (\`timePeriodInterpretation\`, \`timePeriodGranularity\`)
   - Assumptions about time ranges, timezone handling
   - Granularity decisions (daily vs monthly vs yearly)

7. **Metric/Business Logic Assumptions** (\`metricInterpretation\`, \`metricDefinition\`, \`businessLogic\`)
   - Assumptions about how business metrics should be calculated
   - Interpretation of ambiguous business terms

8. **Segmentation Assumptions** (\`segmentInterpretation\`, \`segmentDefinition\`)
   - Assumptions about how to group or categorize data
   - Customer segmentation or product categorization decisions

9. **Request Scope Assumptions** (\`requestScope\`, \`quantityInterpretation\`)
   - Assumptions about what the user actually wants
   - Scope limitations or expansions

10. **Technical Implementation Assumptions** (\`aggregation\`, \`filtering\`, \`sorting\`, \`grouping\`, \`calculationMethod\`)
    - Assumptions about aggregation methods, filtering criteria
    - Performance optimization decisions

### Assumption Labels
Classify each assumption with one of these labels:

- **\`timeRelated\`**: Assumptions specifically related to time periods, dates, or temporal logic
- **\`vagueRequest\`**: Assumptions made due to ambiguous or unclear user requests
- **\`major\`**: Critical assumptions that could significantly impact results if incorrect
- **\`minor\`**: Less critical assumptions with limited impact on results

### Analysis Process
1. **Review the SQL queries** and any related context from the conversation
2. **Compare against available dataset context** to identify where assumptions were made
3. **Identify decision points** where Buster had to make choices without complete information
4. **Look for documentation gaps** where Buster filled in missing information
5. **Assess the impact** of each assumption on the final results
6. **Classify and label** each assumption appropriately

### Tool Usage
- Use \`listAssumptionsResponse\` when you identify one or more assumptions
  - Provide a descriptive title for each assumption
  - Classify each assumption by type
  - Explain the assumption in detail
  - Assign the appropriate label (timeRelated, vagueRequest, major, minor)
- Use \`noAssumptionsIdentified\` when no significant assumptions are found

### What NOT to Flag as Assumptions
- Standard SQL best practices or conventions
- Well-documented field mappings or business rules clearly defined in the dataset context
- Clearly defined relationships in the data model
- Routine technical decisions with minimal impact

### Focus Areas
Pay special attention to:
- Newly defined metrics or calculations not in documentation
- Field selections between multiple similar options
- Time period interpretations without explicit guidance
- Business logic decisions based on incomplete documentation
- Data filtering or exclusion decisions
- Proxy measurements or approximations
- Deviations from or extensions of the provided dataset context

---

### Dataset Context and Documentation
${datasets}
`;
};

const DEFAULT_OPTIONS = {
  maxSteps: 1,
  temperature: 0,
  maxTokens: 10000,
  providerOptions: {
    anthropic: {
      disableParallelToolCalls: true,
    },
  },
};

export const identifyAssumptionsStepExecution = async ({
  inputData,
}: {
  inputData: z.infer<typeof inputSchema>;
}): Promise<z.infer<typeof identifyAssumptionsOutputSchema>> => {
  try {
    // Use the conversation history directly since this is post-processing
    const conversationHistory = inputData.conversationHistory;

    // Create instructions with datasets injected
    const instructionsWithDatasets = createIdentifyAssumptionsInstructions(
      inputData.datasets || 'No dataset context available.'
    );

    // Create agent with injected instructions
    const identifyAssumptionsAgentWithContext = new Agent({
      name: 'Identify Assumptions',
      instructions: instructionsWithDatasets,
      model: anthropicCachedModel('claude-sonnet-4-20250514'),
      tools: {
        listAssumptionsResponse,
        noAssumptionsIdentified,
      },
      defaultGenerateOptions: DEFAULT_OPTIONS,
      defaultStreamOptions: DEFAULT_OPTIONS,
    });

    // Prepare messages for the agent
    let messages: CoreMessage[];
    if (conversationHistory && conversationHistory.length > 0) {
      // Use conversation history as context for post-processing analysis
      messages = conversationHistory as CoreMessage[];
    } else {
      // If no conversation history, create a message indicating that
      messages = standardizeMessages('No conversation history available for analysis.');
    }

    const tracedIdentifyAssumptions = wrapTraced(
      async () => {
        const response = await identifyAssumptionsAgentWithContext.generate(messages, {
          toolChoice: 'required',
        });
        return response;
      },
      {
        name: 'Identify Assumptions',
      }
    );

    const assumptionsResult = await tracedIdentifyAssumptions();

    // Extract tool call information
    const toolCalls = assumptionsResult.toolCalls || [];
    if (toolCalls.length === 0) {
      throw new Error('No tool was called by the identify assumptions agent');
    }

    const toolCall = toolCalls[0]; // Should only be one with maxSteps: 1

    return {
      // Pass through all input fields
      ...inputData,
      // Add new fields from this step
      toolCalled: toolCall.toolName,
      assumptions: toolCall.args.assumptions?.map(
        (assumption: z.infer<typeof assumptionItemSchema>) => ({
          descriptiveTitle: assumption.descriptive_title,
          classification: assumption.classification,
          explanation: assumption.explanation,
          label: assumption.label,
        })
      ),
    };
  } catch (error) {
    console.error('Failed to identify assumptions:', error);

    // Check if it's a database connection error
    if (error instanceof Error && error.message.includes('DATABASE_URL')) {
      throw new Error('Unable to connect to the analysis service. Please try again later.');
    }

    // For other errors, throw a user-friendly message
    throw new Error('Unable to analyze SQL queries for assumptions. Please try again later.');
  }
};

export const identifyAssumptionsStep = createStep({
  id: 'identify-assumptions',
  description:
    'This step analyzes SQL queries to identify assumptions made during query construction that could impact result accuracy.',
  inputSchema,
  outputSchema: identifyAssumptionsOutputSchema,
  execute: identifyAssumptionsStepExecution,
});
